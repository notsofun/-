{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10718639,"sourceType":"datasetVersion","datasetId":6643917},{"sourceId":10718703,"sourceType":"datasetVersion","datasetId":6643967},{"sourceId":10718706,"sourceType":"datasetVersion","datasetId":6643970}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\ndf_generated = pd.read_csv('/kaggle/input/first-fine-tuned/generated_results_first_finetued.csv')\ndf_standard = pd.read_csv('/kaggle/input/standard/_.csv')\n\n\nmerged_df = pd.merge(df_generated,df_standard,on='id')\nprint(merged_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:32:15.983286Z","iopub.execute_input":"2025-02-11T05:32:15.983634Z","iopub.status.idle":"2025-02-11T05:32:16.026681Z","shell.execute_reply.started":"2025-02-11T05:32:15.983603Z","shell.execute_reply":"2025-02-11T05:32:16.025751Z"}},"outputs":[{"name":"stdout","text":"    id              question  \\\n0  296     你觉得自己是个擅长放下过去的人吗？   \n1   55  你觉得自己三年前和现在最大的不同是什么？   \n2  143      你喜欢听鬼故事吗？还是胆子很小？   \n3  142     你觉得“脑洞大”是个优点还是缺点？   \n4   99       你对理想的另一半有什么要求吗？   \n\n                                            response             questions  \\\n0           嗯，我以前觉得没太大问题，但后来才明白，现在才明白，其实他还是个值得尊敬的人呢。     你觉得自己是个擅长放下过去的人吗？   \n1  嗯，这个我得好好想想。三年前和现在最大的不同，我得好好想想。第一反应是年龄，三年前是18岁，...  你觉得自己三年前和现在最大的不同是什么？   \n2                   嗯，我得好好想想。我最喜欢听科幻片，尤其是科幻片，有科幻的氛围。      你喜欢听鬼故事吗？还是胆子很小？   \n3  嗯，这个电影我看过，是一部由日本电影公司东宝影业发行的电影。影片讲述的是一个叫黑手的少年在面...     你觉得“脑洞大”是个优点还是缺点？   \n4                        嗯，这个问题挺有意思的，你了解他的呢？他是哪年出生的？       你对理想的另一半有什么要求吗？   \n\n                              answers  \n0                        不是，需要时间慢慢消化。  \n1  三年前更偏向于“急功近利”，现在学会了更多享受过程和关注内心的变化。  \n2                          喜欢，但听完会后悔。  \n3                      绝对是优点，能带来很多乐趣。  \n4                    理解我，能聊得来，有自己的世界。  \n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\n\n# model = SentenceTransformer(\"all-MiniLM-L6-v2\")  # 轻量级模型，可本地运行\n\n# question = \"你有没有一首歌是“完全听不懂但很喜欢”？\"\n# reference_answer = \"有，比如一些纯音乐或异国歌曲。\"\n# candidate_answer = \"嗯，你说的那也是我呢，好像有些歌我都没听懂。\"\n\n# emb1 = model.encode(reference_answer, convert_to_tensor=True)\n# emb2 = model.encode(candidate_answer, convert_to_tensor=True)\n\n# score = util.pytorch_cos_sim(emb1, emb2)\n# print(score.item())  # 值越接近 1，表示语义越相近\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer, util\nimport torch\n\n# 加载 SBERT 模型\nsbert_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n\n# 计算语义相似度的函数\ndef compute_sbert_similarity(question, reference, candidate):\n    # **批量编码，提高效率**\n    embeddings = sbert_model.encode([question, reference, candidate], convert_to_tensor=True)\n\n    q_embedding, r_embedding, c_embedding = embeddings  # 解包\n\n    # **计算余弦相似度**\n    sim_r = util.pytorch_cos_sim(c_embedding, r_embedding).item()  # 候选回答 vs 参考答案\n    sim_q = util.pytorch_cos_sim(c_embedding, q_embedding).item()  # 候选回答 vs 问题本身（防止无关回答）\n\n    return (sim_r + sim_q) / 2  # 平均相似度\n\n# 示例\nquestion = \"你有没有一首歌是‘完全听不懂但很喜欢’？\"\nreference_answer = \"有，比如一些纯音乐或异国歌曲。\"\ncandidate_answer = \"嗯，你说的那也是我呢，好像有些歌我都没听懂。\"\n\nsbert_score = compute_sbert_similarity(question, reference_answer, candidate_answer)\nprint(f\"SBERT 相似度评分: {sbert_score:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:24:14.478279Z","iopub.execute_input":"2025-02-11T05:24:14.478579Z","iopub.status.idle":"2025-02-11T05:24:50.595729Z","shell.execute_reply.started":"2025-02-11T05:24:14.478547Z","shell.execute_reply":"2025-02-11T05:24:50.594848Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08231d922ab34ed4aa6847d8d4c415a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9febb69070d247b98eff30ce97658eca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/4.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"632aeb5b1df04ce7bf0d9e1d5e4cbd2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67f062c813de422cb982a8f1d4523b67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcd749be6b134d6597e673d5b1d6af89"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"798926b7e95d4737a539aa11e20c0ae9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43f4e3b01074459799519d5db3e079a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f88e7f79e5f43a9bddd4932e6b6f516"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"564af879d0c346c9a2d4a4cf35b7b97e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15558a0e0164462695cd629f2ec36525"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b03385eb0454d85905bcd67aa0dd138"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9c40a0a22924a7aa5750c1e13265cd9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c5dc9c7b8d4443396aa046d1d5cec17"}},"metadata":{}},{"name":"stdout","text":"SBERT 相似度评分: 0.6609\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer\n\n# **4️⃣ 加载模型（这里用 BERT，但其实 logits 已经是模型输出了）**\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=2)\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:24:57.466988Z","iopub.execute_input":"2025-02-11T05:24:57.467301Z","iopub.status.idle":"2025-02-11T05:25:05.665724Z","shell.execute_reply.started":"2025-02-11T05:24:57.467270Z","shell.execute_reply":"2025-02-11T05:25:05.665093Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/624 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0026b8c9ae41aba96b82e054bb3f24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/412M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a84a184785c4ede90d37ea9549ad821"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f47f50118e2a4b2dbb2037816b7daf7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4161b438b79b4576b3694f06d517e46a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1864a6386554df2876af7e90c86594e"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# **1️⃣ 读取 JSON 文件**\nwith open(\"/kaggle/input/logit-classification-train/logit_classification_train.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# **2️⃣ 转换为 PyTorch Dataset**\nclass QADataset(Dataset):\n    def __init__(self, data, tokenizer, max_length=128):\n        self.questions = [item[\"question\"] for item in data]\n        self.candidates = [item[\"candidate\"] for item in data]\n        self.labels = [torch.tensor(item[\"label\"], dtype=torch.long) for item in data]\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n        # 预处理数据\n        self.encodings = tokenizer(\n            self.questions, \n            self.candidates, \n            padding=True, \n            truncation=True, \n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        return {\n            \"input_ids\": self.encodings[\"input_ids\"][idx], \n            \"attention_mask\": self.encodings[\"attention_mask\"][idx], \n            \"labels\": self.labels[idx]\n        }\n\ndataset = QADataset(data,tokenizer)\n\n# **3️⃣ 创建 DataLoader**\ntrain_loader = DataLoader(dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:25:25.454528Z","iopub.execute_input":"2025-02-11T05:25:25.454889Z","iopub.status.idle":"2025-02-11T05:25:25.486662Z","shell.execute_reply.started":"2025-02-11T05:25:25.454861Z","shell.execute_reply":"2025-02-11T05:25:25.485873Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import wandb\n\nwb_token = '4dc26b901e88c96bb7e5ed1dc8abb1dfc9988fe9'\nwandb.login(key=wb_token)\nrun = wandb.init(\n project='Zhidian_Fine-tune-逻辑判断', \n job_type='training',\n anonymous='allow'\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:27:05.977877Z","iopub.execute_input":"2025-02-11T05:27:05.978198Z","iopub.status.idle":"2025-02-11T05:27:20.609589Z","shell.execute_reply.started":"2025-02-11T05:27:05.978173Z","shell.execute_reply":"2025-02-11T05:27:20.608928Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhuangzhidian15\u001b[0m (\u001b[33mhuangzhidian15-zurich-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250211_052714-zr5t9lx7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/huangzhidian15-zurich-university/Zhidian_Fine-tune-%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD/runs/zr5t9lx7' target=\"_blank\">desert-energy-1</a></strong> to <a href='https://wandb.ai/huangzhidian15-zurich-university/Zhidian_Fine-tune-%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/huangzhidian15-zurich-university/Zhidian_Fine-tune-%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD' target=\"_blank\">https://wandb.ai/huangzhidian15-zurich-university/Zhidian_Fine-tune-%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/huangzhidian15-zurich-university/Zhidian_Fine-tune-%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD/runs/zr5t9lx7' target=\"_blank\">https://wandb.ai/huangzhidian15-zurich-university/Zhidian_Fine-tune-%E9%80%BB%E8%BE%91%E5%88%A4%E6%96%AD/runs/zr5t9lx7</a>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"\"\"\"开始微调了哦\"\"\"\n# **5️⃣ 训练参数**\ntraining_args = TrainingArguments(\n    output_dir=\"./logit_classified_前后逻辑_model\",\n    per_device_train_batch_size=8,\n    num_train_epochs=3,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_strategy=\"epoch\",\n)\n\n# **6️⃣ 用 Trainer 进行微调**\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset\n)\n\ntrainer.train()\n\n# **7️⃣ 保存模型**\nmodel.save_pretrained(\"./fine_tuned_logit_model\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:27:27.627631Z","iopub.execute_input":"2025-02-11T05:27:27.627918Z","iopub.status.idle":"2025-02-11T05:27:44.529057Z","shell.execute_reply.started":"2025-02-11T05:27:27.627896Z","shell.execute_reply":"2025-02-11T05:27:44.528100Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [21/21 00:13, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>0.383600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.084100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(\"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\")\n\nfrom huggingface_hub import HfApi\n\napi = HfApi()\nmodel_name = \"Zhidian2025/bert-base-chinese-logitrelevancy-finetuned\"  # 改成你的用户名\napi.create_repo(model_name, exist_ok=True)  # 创建一个新的 Hugging Face 公开仓库\n\napi.upload_folder(\n    folder_path=\"./fine_tuned_logit_model\",  # 你的微调模型路径\n    repo_id=model_name,  # 你的 Hugging Face 仓库\n    repo_type=\"model\"\n)\nprint('模型上传成功')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-11T05:29:11.497360Z","iopub.execute_input":"2025-02-11T05:29:11.497682Z","iopub.status.idle":"2025-02-11T05:29:31.167271Z","shell.execute_reply.started":"2025-02-11T05:29:11.497658Z","shell.execute_reply":"2025-02-11T05:29:31.166544Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/409M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc028187365e4a7589091aef78839975"}},"metadata":{}},{"name":"stdout","text":"模型上传成功\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import torch\n\ndef compute_final_score(row):\n    question = row[\"question\"]\n    reference = row[\"reference\"]\n    candidate = row[\"candidate\"]\n\n    # **Step 1: 计算 SBERT 相似度**\n    sbert_score = compute_sbert_similarity(question, reference, candidate)\n\n    # **Step 2: Fine-tuned BERT 分类（回答是否相关）**\n    inputs = tokenizer(\n        question, candidate,\n        return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128\n    )\n\n    # **确保数据在相同设备上**\n    inputs = {key: value.to(model.device) for key, value in inputs.items()}\n\n    # **前向传播计算 logits**\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    # **获取 \"回答正确\" 概率**\n    answer_validity = torch.softmax(logits, dim=-1)[0, 1].item()\n\n    # **Step 3: 计算最终综合评分**\n    alpha = 0.7  # 语义相似度权重\n    final_score = alpha * sbert_score + (1 - alpha) * answer_validity\n\n    return final_score","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# **加载 Fine-tuned BERT 模型**（用于分类任务）\nmodel_name = \"Zhidian2025/bert-base-chinese-logitrelevancy-finetuned\"  # 这里换成你 Fine-tuned 的模型\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\n\n# **将模型移动到 GPU（如果可用）**\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# **在 DataFrame 中应用评分计算**\nmerged_df[\"FinalScore\"] = merged_df.apply(compute_final_score, axis=1)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"merged_df['FinalScore'] = merged_df['FinalScore'].apply(lambda x: x.item() if hasattr(x, 'item') else x)","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 输出结果\nprint(merged_df.describe())\n\nmerged_df.to_csv('Evalution_fine_tuned_model_base.csv')","metadata":{},"outputs":[],"execution_count":null}]}